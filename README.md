<html><head>
    <title>Rupam's Website</title>

    <link rel="stylesheet" type="text/css" href="/css/main.css">
  <script charset="utf-8" src="https://platform.twitter.com/js/button.d6f0e03b97fa3e281bb07d1de2c3bee3.js"></script></head>
  <body>
    <nav>
<ul>
<li><a href="/">Home</a></li>
<li><a href="/index.html#Publications">Publications</a></li>
<li><a href="/index.html#Software">Software</a></li>
<li><a href="/files/rupam-cv.pdf">CV</a></li>
<li><a href="/blog">Blog</a></li>
<li><a href="/rl-robots-course">RL with Robots course</a></li>
</ul>
    </nav>
<div class="container">


<h4 id="a-rupam-mahmood">A. Rupam Mahmood</h4>


		<span class="body" style="font-size:12px;">
	<p>
<!--		<span style="font-size:12px;">A. Rupam Mahmood </span><br /> -->
			<img alt="Rupam" src="files/Rupam.jpg" width="250" align="right">
    <br>
        <b>Assistant Professor</b><br>
        <b>Director of <a href="https://spaces.facsci.ualberta.ca/rlai/">RLAI</a>; Fellow at <a href="https://www.amii.ca/">Amii</a> </b><br>
        <a href="https://www.ualberta.ca/computing-science/">Department of Computing Science</a><br>
        University of Alberta<br>
        Email: <span style="font-family:sans-serif">armahmood@ualberta.ca</span><br>
        <a href="https://scholar.google.com/citations?user=YwB8XM4AAAAJ&amp;hl=en">Google Scholar Profile</a>
    <br>
    <br>



</p><hr>

<h4 id="about-me">About Me</h4>

<p>I develop reinforcement learning algorithms and real-time learning systems for controlling physical robots.
My research objective is developing a computational and scientific understanding of general-purpose mind-like systems for robots.
</p>

<hr>

    <div id="Teaching"></div>

    <h4 id="Teaching">Teaching</h4>


    <p>I am teaching a course on <a href="/rl-robots-course">reinforcement learning
      with robots.</a>
    In this course, graduate students learn how to develop control methods
    that they can
    evaluate in their own created worlds by understanding the fundamentals of MDPs,
    iterative methods, stochastic approximation methods and policy gradient methods.
    Then they apply their developed methods to learn to control physical robots.
    Enroute, they develop tools and understanding for using physical robots
    nearly as easily as the simulated ones.
    </p>

    <hr>


    <div id="Publications"></div>

    <h4 id="publications">Publications</h4>

    <p>Korenkevych, D., Mahmood, A. R., Vasan, G., Bergstra, J. (2019).
    <a href="https://arxiv.org/abs/1903.11524">Autoregressive policies for continuous control deep reinforcement learning</a>.
    In <em>Proceedings of the 28th International Joint Conference on Artificial Intelligence</em>.
    <a href="https://youtu.be/NCpyXBNqNmw">Companion video</a> and
    <a href="https://github.com/kindredresearch/arp">source code</a>.
    </p>


    <p>Mahmood, A. R., Korenkevych, D., Vasan, G., Ma, W., Bergstra, J. (2018).
    <a href="files/MKVMB-Benchmarking-2018.pdf">Benchmarking reinforcement learning algorithms on real-world robots</a>.
    In <em>Proceedings of the 2nd Annual Conference on Robot Learning (CoRL)</em>.
    <a href="https://arxiv.org/abs/1809.07731">arXiv</a>.
    <a href="https://www.youtube.com/watch?v=ovDfhvjpQd8">Companion video</a> and
    <a href="https://github.com/kindredresearch/SenseAct">source code</a>.
  </p>

    <p>Mahmood A. R.,  Korenkevych, D., Komer, B. J., Bergstra, J. (2018).
    <a href="https://arxiv.org/abs/1803.07067">Setting up a reinforcement learning task with a real-world robot</a>.
    In <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>.
    <a href="https://www.youtube.com/watch?v=ZVIxt2rt1_4">Companion video</a> and
    <a href="https://github.com/kindredresearch/SenseAct">source code</a>.
  </p>

  <p>Yu, H., Mahmood, A. R., Sutton, R. S. (2018).
  <a href="http://www.jmlr.org/papers/volume19/17-283/17-283.pdf"> On generalized Bellman equations and temporal-difference learning</a>.
    <em>Journal of Machine Learning Research (JMLR) 19</em>(48):1-49.
  </p>

    <p>Mahmood, A. R. (2017).
    <a href="https://era.library.ualberta.ca/files/cbc386j54q/Mahmood_Ashique_201709_PhD.pdf"><em>Incremental Off-policy Reinforcement Learning Algorithms</em></a>.
    PhD thesis, Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8.</p>

    <p>Yu, H., Mahmood, A. R., Sutton, R. S. (2017).
    On generalized Bellman equations and temporal-difference learning.
    In <em>Proceedings of the 30th Canadian Conference on Artificial Intelligence (CAI)</em>, Edmonton, Canada.
    <a href="https://arxiv.org/pdf/1704.04463.pdf"> arXiv</a>.
    </p>

    <p>Mahmood, A. R., Yu, H., Sutton, R. S. (2017).
    <a href="files/MYS-ABQ-2017.pdf">Multi-step off-policy learning without importance sampling ratios</a>.
    arXiv:1702.03006.</p>

    <p>Sutton, R. S., Mahmood, A. R., White, M. (2016).
    <a href="files/SMW-emphasis-2016.pdf">An emphatic approach to the problem of off-policy temporal-difference learning</a>.
    <em>Journal of Machine Learning Research (JMLR) 17</em>(73):1-29.</p>

    <p>van Seijen, H., Mahmood, A. R., Pilarski, P. M., Machado, M. C., Sutton, R. S. (2016).
    <a href="https://arxiv.org/abs/1512.04087">True online temporal-difference learning</a>.
    <em>Journal of Machine Learning Research (JMLR) 17</em>(1):5057-5096.
    Code for <a href="https://github.com/armahmood/totd-rndmdp-experiments">random MDP experiments</a>.</p>

    <p>Mahmood, A. R., Sutton, R. S. (2015).
    <a href="files/MS-WIS-O(n)-UAI-2015.pdf">Off-policy learning based on weighted importance sampling with linear computational complexity</a>.
    In <em>Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence (UAI)</em>, Amsterdam, Netherlands.
    Code for <a href="https://github.com/armahmood/usage-td-experiments">on-policy</a>
    and <a href="https://github.com/armahmood/wis-td-experiments">off-policy experiments</a>.</p>

    <p>Mahmood, A. R., Yu, H., White, M., Sutton, R. S. (2015).
    <a href="files/MYWS-EWRL-2015.pdf">Emphatic temporal-difference learning</a>.
    In <em>the 2015 European Workshop on Reinforcement Learning (EWRL)</em>. arXiv:1507.01569.</p>

    <p>van Seijen, H., Mahmood, A. R., Pilarski, P. M., Sutton, R. S. (2015).
    <a href="files/vSMPS-EWRL-2015.pdf">An empirical evaluation of true online TD(λ)</a>.
    In <em>the 2015 European Workshop on Reinforcement Learning (EWRL)</em>. arXiv:1507.00353.
    Code for <a href="https://github.com/armahmood/totd-rndmdp-experiments">random MDP experiments</a>.</p>

    <p>Mahmood, A. R., van Hasselt, H., Sutton, R. S. (2014).
    <a href="files/MvHS-NeurIPS-2014.pdf">Weighted importance sampling for off-policy learning with linear function approximation</a>.
    <em>Advances in Neural Information Processing Systems (NeurIPS) 27</em>, Montreal, Canada.
    <a href="files/wislstdpseudocode.pdf">Pseudo-code</a> and
    <a href="https://github.com/armahmood/wislstd-experiments">Code for experiments</a>.</p>

    <p>van Hasselt, H., Mahmood, A. R., Sutton, R. S. (2014).
    <a href="files/vHMS-UAI-2014.pdf">Off-policy TD(λ) with a true online equivalence</a>.
    In <em>Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence (UAI)</em>, Quebec City, Canada.</p>

    <p>Sutton, R. S., Mahmood, A. R. , Precup, D., van Hasselt, H. (2014).
    <a href="files/SMPvH-ICML-2014.pdf">A new Q(λ) with interim forward view and Monte Carlo equivalence</a>.
    In <em>Proceedings of the 31st International Conference on Machine Learning (ICML)</em>, Beijing, China.</p>

    <p>Mahmood, A. R., Sutton, R. S. (2013).
    <a href="files/MS-RepSearch-AAAI-WS-2013.pdf">Representation Search through Generate and Test</a>.
    In <em>Proceedings of the AAAI Workshop on Learning Rich Representations from Low-Level Sensors</em>, Bellevue, Washington, USA.</p>

    <p>Mahmood, A. R., Sutton, R. S., Degris, T., Pilarski, P. M. (2012).
    <a href="files/MSDP-Autostep-ICASSP-2012.pdf">Tuning-free step-size adaptation</a>.
    In <em>Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, Kyoto, Japan.
    <a href="https://github.com/armahmood/nonstationary-experiments">Code</a>.</p>

    <p>Mahmood, A. R. (2011).
    <a href="files/mahmood-TR11-01.pdf">Structure learning of causal bayesian networks: A survey</a>.
    <em>Technical report TR11-01, Department of Computing Science, University of Alberta</em>, Edmonton, AB, Canada T6G 2E8.</p>

    <p>Mahmood, A. R. (2010).
    <a href="files/Mahmood-MSc-Thesis-2010.pdf"><em>Automatic Step-size Adaptation in Incremental Supervised Learning</em></a>.
    Master’s thesis, Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8.</p>


<hr>

<div id="Software"></div>

<h4 id="software">Software</h4>

<p><a href="https://github.com/kindredresearch/SenseAct">A computational framework and a benchmark task suite</a>
 for developing and evaluating reinforcement learning methods with physical robots.</p>


<p><a href="http://www.github.com/armahmood/totd-rndmdp-experiments">A platform</a>
written in Python for running model-free policy-evaluation
experiments on randomly generated Markov Decision Processes.</p>

<p><a href="http://www.github.com/armahmood/Create-Serial-Port-Packet-Processor">CSP3</a>:
an RL platform for iRobot Create Robots written in C for extracting data
with minimal delay streamed by the robot and running simultaneously
a reinforcement learning agent using the data.</p>

<hr>
<br>
<p>
<a href="files/probabilities-expectations.pdf">A tutorial on probabilities and expectations</a>
</p>
<br>



<!--
***

#### Thoughts

Some high-level thoughts on how to understand and develop minds.
-->




	<!-- Start of StatCounter Code for Default Guide -->
	<script type="text/javascript">
	var sc_project=8622283;
	var sc_invisible=1;
	var sc_security="57666372";
	var scJsHost = (("https:" == document.location.protocol) ?
	"https://secure." : "http://www.");
	document.write("<sc"+"ript type='text/javascript' src='" +
	scJsHost+
	"statcounter.com/counter/counter.js'></"+"script>");
	</script><script type="text/javascript" src="https://secure.statcounter.com/counter/counter.js"></script>
	<noscript><div class="statcounter"><a title="web analytics"
	href="http://statcounter.com/" target="_blank"><img
	class="statcounter"
	src="http://c.statcounter.com/8622283/0/57666372/1/"
	alt="web analytics"></a></div></noscript>
	<!-- End of StatCounter Code for Default Guide -->




</span></div>

  <footer>
   <ul>
   </ul>
  </footer>
  
  <iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 63px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.d942dbac55d395b6a752976f272a24f6.en.html#dnt=false&amp;id=twitter-widget-0&amp;lang=en&amp;screen_name=rupammahmood&amp;show_count=false&amp;show_screen_name=false&amp;size=m&amp;time=1574150875331" data-screen-name="rupammahmood"></iframe>
  <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<iframe scrolling="no" frameborder="0" allowtransparency="true" src="https://platform.twitter.com/widgets/widget_iframe.d942dbac55d395b6a752976f272a24f6.html?origin=https%3A%2F%2Farmahmood.github.io" title="Twitter settings iframe" style="display: none;"></iframe></body></html>
